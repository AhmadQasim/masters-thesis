% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Introduction}\label{chapter:introduction}
Recent success of deep learning methods rely heavily on large amounts of well annotated training data \cite{sun2017}. Especially for biomedical images, annotations are scarce as they crucially depend on the availability of trained experts whose time is often expensive and limited. Active learning algorithms are designed to address this issue by finding the most informative images for annotation \cite{settles2009, sadafi2019, joshi2009}, but are mostly benchmarked on natural image datasets such as ImageNet \cite{gal2016, ducoffe2015, holub2008}. Biomedical images however differ in their characteristics from natural images. They are typically not as diverse in terms of color range and often they are classified by only small feature variations, e.g. in texture and size \cite{matek2019, esteva2017}. Moreover, biomedical image datasets are often imbalanced, containing rare classes, which can significantly influence the diagnosis. Active learning has been shown to work in biomedical image classification tasks \cite{sadafi2019, smailagic2018} and image segmentation \cite{yang2017}. However it is not clear which particular active learning algorithm will be the most suitable for different biomedical image data and how the performance can be improved by combining it with other deep learning methods.

Pre-training methods such as transfer learning and self-supervised pre-training show a great potential for being used as the network's initial weights to improve the network performance on classification tasks involving low number of labeled images \cite{chen2020, oord2018, newell2020, sagheer2019}. Here, a network uses representation from another, ideally similar dataset (i.e. transfer learning), or it learns a representation without incorporating any labels  (self-supervised learning) \cite{jing2020}. The most common transfer learning method is to use pre-trained ImageNet weights. This method has been used in many biomedical applications to initialize deep learning models \cite{rajpurkar2017, wang2017}. However Raghu and Zhang et al. \cite{raghu2019} showed that in several biomedical imaging applications, transfer learning from ImageNet does not lead to better results. Furthermore, self-supervised learning has recently been shown to be effective for improving classification performance on biomedical images \cite{holmberg2020}. 

Finally, semi-supervised learning uses unlabeled data to increase the performance as well as the stability of predictions \cite{sohn2020, tarvainen2017}. In the field of biomedical imaging, many applications leverage high-throughput technology \cite{blasi2016} to generate large quantities of unlabelled data, whereas, as discussed, annotations are typically scarce. Thus, the paradigm of semi-supervised learning is particularly appealing in this domain.

% rework this a bit to talk about the comparison rather than the combination
In this thesis, firstly, different active learning algorithms are compared on a challenging biomedical image dataset. Secondly, the results of the best algorithm are improved by adding pre-training and semi-supervised learning. To prove that whether this combination of active learning algorithm, pre-training and training strategy always works, an extensive grid-search is performed on three active learning algorithms plus random sampling (baseline), three pre-training methods plus random initialization (baseline), and two training strategies including supervised and semi-supervised learning on three exemplary biomedical image data sets. As the result of this investigation, an optimal strategy for incomplete-supervision biomedical image data is found.

% expand this and get help from http://burrsettles.com/pub/settles.activelearning.pdf
\section{Active learning}
Active learning\cite{settles2009} tries to mitigate the cost of annotating the datasets, by efficiently selecting the data points, which can be annotated for maximum performance gain on the underlying task. \\
The performance of semi-supervised or unsupervised learning does not surpass the performance of fully-supervised learning, on a dataset of a fixed size. More annotated data results in superior performance, but obtaining annotations has high labor and time costs. The cost of annotations varies depending upon the underlying deep learning task. Compared to classification tasks, segmentation tasks have significantly higher costs as the annotation process has to be carried out on the pixel-level. Similarly, detection requires labor and time intensive bounding boxes to be drawn over the target images. The cost of annotation gets even high in the biomedical domain. Due to the safety critical nature of medical data, multiple experts have to be consulted for annotating biomedical datasets. \\
Hence, by selecting the data points which can have the most positive effect on the model's performance, active learning reduces the cost of data annotation.

% make this bigger and take help from https://link.springer.com/article/10.1007/s10994-019-05855-6
\section{Semi-supervised learning}
Semi-supervised learning (SSL)\cite{van2020} mitigates the requirement for labeled data by providing a means of leveraging unlabeled data. SSL operates on the assumption that the unlabeled data contains some prior information about the labeled data. Since unlabeled data can often be obtained with minimal human labor, any performance boost conferred by SSL often comes with low cost. \\